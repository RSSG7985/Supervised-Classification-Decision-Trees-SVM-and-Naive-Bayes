{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer:Information Gain is an important concept used in Decision Tree algorithms to decide how the data should be split at each node of the tree. It measures how much information a feature provides about the target class and helps in selecting the most suitable attribute for classification.\n",
        "\n",
        "Information Gain is based on the concept of entropy, which represents the degree of uncertainty or impurity in a dataset. A dataset with mixed classes has higher entropy, while a dataset with a single class has zero entropy. The main objective of a Decision Tree is to reduce this uncertainty step by step.\n",
        "\n",
        "Information Gain is calculated as the difference between the entropy of the original dataset and the weighted average entropy after splitting the dataset based on a particular feature. If a feature results in a large reduction in entropy, it means that the feature is effective in separating the data into well-defined classes.\n",
        "\n",
        "Mathematically, Information Gain can be expressed as:\n",
        "\n",
        "Information Gain = Entropy (before split) − Entropy (after split)\n",
        "\n",
        "In Decision Trees, the Information Gain is calculated for all available features at a node. The feature with the highest Information Gain is selected for splitting because it creates the most homogeneous child nodes. This process is repeated recursively until a stopping condition is reached, such as all data belonging to the same class or no features remaining.\n",
        "\n",
        "Information Gain plays a crucial role in building efficient Decision Trees by ensuring that each split improves the purity of the data. It is widely used in popular Decision Tree algorithms such as ID3 and C4.5 to achieve accurate and interpretable classification models.\n",
        "\n",
        "\n",
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Answer: Gini Impurity and Entropy are two commonly used impurity measures in Decision Tree algorithms. Both are used to evaluate how well a dataset is split into different classes, but they differ in their calculation method, interpretation, and practical usage.\n",
        "\n",
        "Entropy measures the level of randomness or uncertainty present in a dataset. It originates from information theory and indicates how unpredictable the class distribution is. A dataset with completely mixed classes has high entropy, while a dataset containing only one class has zero entropy. Entropy considers the probability distribution of all classes and gives more weight to rare class occurrences, making it sensitive to small changes in class proportions.\n",
        "\n",
        "Gini Impurity, on the other hand, measures the probability of incorrectly classifying a randomly chosen data point if it were labeled according to the class distribution of the dataset. It focuses on how often a randomly selected element would be misclassified. Gini Impurity is computationally simpler than entropy and does not involve logarithmic calculations, which makes it faster to compute.\n",
        "\n",
        "From a performance perspective, Gini Impurity generally favors splits that create larger, purer partitions, while Entropy tends to produce more balanced trees. In most practical scenarios, both measures lead to similar splits, but Gini is often preferred when computational efficiency is important, such as in large datasets.\n",
        "\n",
        "In terms of usage, Entropy is mainly used in algorithms like ID3 and C4.5, whereas Gini Impurity is the default criterion in the CART (Classification and Regression Trees) algorithm. Entropy is useful when detailed information gain analysis is required, while Gini Impurity is suitable when faster model training is a priority.\n",
        "\n",
        "In conclusion, both Gini Impurity and Entropy serve the same purpose of measuring node impurity, but they differ in their mathematical approach, computational complexity, and algorithmic preference. The choice between them depends on the dataset size, required accuracy, and computational constraints.\n",
        "\n",
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer: Pre-pruning is a technique used in Decision Trees to prevent the model from becoming overly complex during the training phase. It involves stopping the growth of the decision tree at an early stage before it fully fits the training data. The main purpose of pre-pruning is to avoid overfitting and to improve the model’s ability to generalize to unseen data.\n",
        "\n",
        "In Decision Trees, if the tree is allowed to grow without restrictions, it may learn noise and minor variations in the training dataset. This results in a very deep tree with many branches, which performs well on training data but poorly on new data. Pre-pruning addresses this issue by introducing stopping criteria while the tree is being built.\n",
        "\n",
        "Common pre-pruning methods include limiting the maximum depth of the tree, setting a minimum number of samples required to split a node, defining a minimum number of samples in leaf nodes, or stopping the split if the improvement in impurity reduction (such as Information Gain or Gini reduction) is below a predefined threshold. These constraints control the tree growth and reduce unnecessary splits.\n",
        "\n",
        "The advantage of pre-pruning is that it reduces model complexity, improves training speed, and lowers the risk of overfitting. It also results in simpler and more interpretable trees. However, one limitation of pre-pruning is that it may stop the tree too early, causing underfitting if important patterns in the data are not fully learned.\n",
        "\n",
        "In simple words it can be said that, pre-pruning is an early stopping strategy in Decision Trees that balances model complexity and performance by restricting tree growth during training. It is especially useful when working with large datasets or when model interpretability and generalization are important considerations.\n"
      ],
      "metadata": {
        "id": "RjmjFDKd3bsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "#Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X = data.data          # Feature matrix\n",
        "y = data.target        # Target labels\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create Decision Tree Classifier using Gini Impurity\n",
        "dt_model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = dt_model.feature_importances_\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(feature_names, importances):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98X-Fn0e5aya",
        "outputId": "27cbe2ea-8ee7-4967-94fb-0b3fad07ef48"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of output of Question 4:\n",
        "\n",
        "In this program, a Decision Tree Classifier is trained using Gini Impurity as the splitting criterion. The Iris dataset is used for classification. After training the model, the feature_importances_ attribute is accessed to determine how important each feature is in making decisions. Higher importance values indicate that the feature contributes more to the classification process."
      ],
      "metadata": {
        "id": "yVqLDE205yfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer: Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for classification and regression tasks. The main objective of an SVM is to find an optimal decision boundary that separates data points of different classes with the maximum possible margin. This margin is defined as the distance between the decision boundary and the nearest data points from each class.\n",
        "\n",
        "In SVM, the data points that lie closest to the decision boundary are known as support vectors. These points are critical in defining the position and orientation of the separating hyperplane. Unlike other classification algorithms that consider all data points, SVM focuses only on the support vectors, which makes it efficient and robust.\n",
        "\n",
        "SVM can handle both linearly separable and non-linearly separable data. For linearly separable data, SVM constructs a straight hyperplane. For non-linear data, SVM uses kernel functions to transform the input data into a higher-dimensional space where a linear separation becomes possible. Commonly used kernels include linear, polynomial, radial basis function (RBF), and sigmoid kernels.\n",
        "\n",
        "One of the key advantages of SVM is its ability to perform well in high-dimensional spaces and with limited training data. It also helps in reducing overfitting by maximizing the margin between classes. However, SVM can be computationally expensive for very large datasets and requires careful selection of kernel parameters.\n",
        "\n",
        "In general words, Support Vector Machine is a highly effective machine learning algorithm that aims to create the best possible separation between classes by maximizing the margin, making it suitable for complex classification problems in real-world applications.\n",
        "\n",
        "\n",
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer: The Kernel Trick is an important concept used in Support Vector Machines (SVM) to handle non-linearly separable data. In many real-world problems, data points cannot be separated using a straight line or a linear hyperplane in their original feature space. The Kernel Trick allows SVM to overcome this limitation without explicitly transforming the data into a higher-dimensional space.\n",
        "\n",
        "The basic idea behind the Kernel Trick is to apply a mathematical function, called a kernel function, which computes the similarity between data points in a transformed feature space. Instead of performing an explicit transformation of the input data, the kernel function enables SVM to operate as if the data were mapped into a higher-dimensional space. This makes it possible to find a linear separating hyperplane in that transformed space, which corresponds to a non-linear boundary in the original space.\n",
        "\n",
        "Common kernel functions used in SVM include the Linear kernel, Polynomial kernel, Radial Basis Function (RBF) kernel, and Sigmoid kernel. Each kernel is suitable for different types of data patterns. For example, the RBF kernel is effective when the relationship between features and classes is complex and non-linear, while the linear kernel is preferred for high-dimensional data with a relatively simple structure.\n",
        "\n",
        "The main advantage of the Kernel Trick is that it reduces computational complexity. Explicitly mapping data to a higher-dimensional space can be computationally expensive or even infeasible. The Kernel Trick avoids this by computing inner products directly in the transformed space, making SVM both efficient and powerful.\n",
        "\n"
      ],
      "metadata": {
        "id": "of59hDja6EVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "#kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "\n",
        "# Calculate accuracy for Linear Kernel\n",
        "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy for RBF Kernel\n",
        "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy using Linear Kernel:\", linear_accuracy)\n",
        "print(\"Accuracy using RBF Kernel:\", rbf_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzBL4AEB6hnw",
        "outputId": "453fb2a2-c9ee-4db8-9e78-553769f7bdd0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using Linear Kernel: 1.0\n",
            "Accuracy using RBF Kernel: 0.8055555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "Answer: The Naïve Bayes classifier is a supervised machine learning algorithm based on Bayes’ Theorem and is widely used for classification tasks such as text classification, spam detection, and sentiment analysis. It predicts the class of a data instance by calculating the probability of each class given the input features and selecting the class with the highest probability.\n",
        "\n",
        "The working principle of the Naïve Bayes classifier relies on Bayes’ Theorem, which describes the relationship between prior probability, likelihood, and posterior probability. The classifier computes the probability of a class given the observed features by combining the prior probability of the class with the likelihood of the features occurring in that class.\n",
        "\n",
        "The term “Naïve” is used because the classifier makes a strong assumption of conditional independence among features. This means it assumes that all input features are independent of each other given the class label. In reality, this assumption is often not true, as features in real-world data can be correlated. Despite this simplification, Naïve Bayes performs surprisingly well in many practical applications.\n",
        "\n",
        "One of the major advantages of Naïve Bayes is its simplicity and computational efficiency. It requires very little training data and works well with high-dimensional datasets. It is also robust to irrelevant features and performs well in real-time prediction scenarios. However, its main limitation is the independence assumption, which can reduce accuracy when features are highly correlated.\n",
        "\n",
        "In conclusion, the Naïve Bayes classifier is a probabilistic classification algorithm that is called “Naïve” due to its assumption of feature independence. Even with this assumption, it remains an effective and popular method for many classification problems.\n",
        "\n",
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "\n",
        "Answer: Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes are three variants of the Naïve Bayes classifier. They differ mainly in the type of data they are designed to handle and the probability distribution they assume for the features.\n",
        "\n",
        "Gaussian Naïve Bayes is used when the input features are continuous and assumed to follow a normal (Gaussian) distribution. For each feature and class, the algorithm calculates the mean and variance, which are then used to estimate the likelihood of a data point belonging to a particular class. This variant is commonly applied in problems involving numerical data, such as medical measurements or sensor readings.\n",
        "\n",
        "Multinomial Naïve Bayes is suitable for discrete data, especially when features represent counts or frequencies. It is widely used in text classification tasks, where features correspond to the number of times a word appears in a document. Multinomial Naïve Bayes considers the frequency of features, making it effective for applications like spam filtering and document categorization.\n",
        "\n",
        "Bernoulli Naïve Bayes is designed for binary or boolean features, where each feature indicates the presence or absence of a characteristic. Unlike Multinomial Naïve Bayes, it does not consider feature frequency but only whether a feature occurs or not. This makes it useful for text classification problems where binary feature vectors are used.\n",
        "\n",
        "In summary, the key differences among these three variants lie in the nature of the input data and the assumed probability distribution. Gaussian Naïve Bayes handles continuous features, Multinomial Naïve Bayes works with count-based data, and Bernoulli Naïve Bayes is best suited for binary features. Choosing the appropriate variant depends on the structure and type of the dataset being analyzed.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NUbQAg7865Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10: Breast Cancer Dataset\n",
        "#Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "#dataset and evaluate accuracy.\n",
        "#Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "#sklearn.datasets.\n",
        "#(Include your Python code and output in the code box below.)\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data      # Features\n",
        "y = data.target    # Target labels\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the classifier\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy of Gaussian Naïve Bayes classifier:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIuLO_HV7kgf",
        "outputId": "ac3f4758-7ef3-48e5-e48e-9ff5ad0e0df2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes classifier: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of output of question 10:\n",
        "\n",
        "In this program, a Gaussian Naïve Bayes classifier is trained on the Breast Cancer dataset from sklearn.datasets. The dataset is split into training and testing sets to evaluate the model’s performance. After fitting the model, predictions are made on the test set, and the accuracy_score function is used to measure how well the classifier performs. The high accuracy indicates that Gaussian Naïve Bayes is effective in distinguishing between malignant and benign tumors in this dataset."
      ],
      "metadata": {
        "id": "QANapAoE78hW"
      }
    }
  ]
}